{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification using Keras with Neptune tracking\n",
    "Notebook inspired from https://keras.io/examples/nlp/text_classification_from_scratch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Neptune) Import Neptune and initialize a project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NEPTUNE_PROJECT\"] = \"showcase/project-text-classification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune.new as neptune\n",
    "\n",
    "project = neptune.init_project()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "We are using the IMDB sentiment analysis data available at https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz. For the purposes of this demo, we've uploaded this data to S3 at https://neptune-examples.s3.us-east-2.amazonaws.com/data/text-classification/aclImdb_v1.tar.gz and will be downloading it from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Neptune) Track datasets using Neptune\n",
    "Since this dataset will be used among all the runs in the project, we track it at the project level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project[\"keras/data/files\"].track_files(\n",
    "    \"s3://neptune-examples/data/text-classification/aclImdb_v1.tar.gz\"\n",
    ")\n",
    "project.sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Neptune) Download files from S3 using Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading data...\")\n",
    "project[\"keras/data/files\"].download(\"..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.extract_files(source=\"../aclImdb_v1.tar.gz\", destination=\"..\")\n",
    "utils.prep_data(imdb_folder=\"../aclImdb\", dest_path=\"../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Neptune) Upload dataset sample to Neptune project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "base_namespace = \"keras/data/sample/\"\n",
    "\n",
    "project[base_namespace][\"train/pos\"].upload(\n",
    "    f\"../data/train/pos/{random.choice(os.listdir('../data/train/pos'))}\"\n",
    ")\n",
    "project[base_namespace][\"train/neg\"].upload(\n",
    "    f\"../data/train/neg/{random.choice(os.listdir('../data/train/neg'))}\"\n",
    ")\n",
    "project[base_namespace][\"test/pos\"].upload(\n",
    "    f\"../data/test/pos/{random.choice(os.listdir('../data/test/pos'))}\"\n",
    ")\n",
    "project[base_namespace][\"test/neg\"].upload(\n",
    "    f\"../data/test/neg/{random.choice(os.listdir('../data/test/neg'))}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training, validation, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params = {\n",
    "    \"batch_size\": 32,\n",
    "    \"validation_split\": 0.2,\n",
    "    \"max_features\": 20000,\n",
    "    \"embedding_dim\": 128,\n",
    "    \"sequence_length\": 500,\n",
    "    \"seed\": 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Neptune) Log data metadata to Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = neptune.init_run(name=\"Keras text classification\", tags=[\"keras\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"data/params\"] = data_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_ds, raw_val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"../data/train\",\n",
    "    batch_size=data_params[\"batch_size\"],\n",
    "    validation_split=data_params[\"validation_split\"],\n",
    "    subset=\"both\",\n",
    "    seed=data_params[\"seed\"],\n",
    ")\n",
    "\n",
    "raw_test_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    \"../data/test\", batch_size=data_params[\"batch_size\"]\n",
    ")\n",
    "\n",
    "print(f\"Number of batches in raw_train_ds: {raw_train_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_val_ds: {raw_val_ds.cardinality()}\")\n",
    "print(f\"Number of batches in raw_test_ds: {raw_test_ds.cardinality()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(stripped_html, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
    "\n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=data_params[\"max_features\"],\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=data_params[\"sequence_length\"],\n",
    ")\n",
    "\n",
    "text_ds = raw_train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    return vectorize_layer(text), label\n",
    "\n",
    "\n",
    "# Vectorize the data.\n",
    "train_ds = raw_train_ds.map(vectorize_text)\n",
    "val_ds = raw_val_ds.map(vectorize_text)\n",
    "test_ds = raw_test_ds.map(vectorize_text)\n",
    "\n",
    "# Do async prefetching / buffering of the data for best performance on GPU.\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=10)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=10)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Neptune) Create a new model and model version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptune.new.exceptions import NeptuneModelKeyAlreadyExistsError\n",
    "\n",
    "project_key = project[\"sys/id\"].fetch()\n",
    "\n",
    "try:\n",
    "    model = neptune.init_model(name=\"keras\", key=\"KER\")\n",
    "    model.stop()\n",
    "except NeptuneModelKeyAlreadyExistsError:\n",
    "    # If it already exists, we don't have to do anything.\n",
    "    pass\n",
    "\n",
    "model_version = neptune.init_model_version(model=f\"{project_key}-KER\", name=\"keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"dropout\": 0.5,\n",
    "    \"strides\": 3,\n",
    "    \"activation\": \"relu\",\n",
    "    \"kernel_size\": 7,\n",
    "    \"loss\": \"binary_crossentropy\",\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"metrics\": [\"accuracy\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version[\"params\"] = model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = utils.build_model(model_params, data_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Neptune) Initialize the Neptune callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neptune.new.integrations.tensorflow_keras import NeptuneCallback\n",
    "\n",
    "neptune_callback = NeptuneCallback(run=run, log_model_diagram=True, log_on_batch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = {\n",
    "    \"epochs\": 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using the train and test datasets.\n",
    "keras_model.fit(\n",
    "    train_ds, validation_data=val_ds, epochs=training_params[\"epochs\"], callbacks=neptune_callback\n",
    ")\n",
    "# Training parameters are logged automatically to Neptune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, curr_model_acc = keras_model.evaluate(test_ds, callbacks=neptune_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Neptune) Associate run with model and vice-versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_meta = {\n",
    "    \"id\": run[\"sys/id\"].fetch(),\n",
    "    \"name\": run[\"sys/name\"].fetch(),\n",
    "    \"url\": run.get_url(),\n",
    "}\n",
    "\n",
    "print(run_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version[\"run\"] = run_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version_meta = {\n",
    "    \"id\": model_version[\"sys/id\"].fetch(),\n",
    "    \"name\": model_version[\"sys/name\"].fetch(),\n",
    "    \"url\": model_version.get_url(),\n",
    "}\n",
    "\n",
    "print(model_version_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run[\"training/model/meta\"] = model_version_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Neptune) Upload serialized model and model weights to Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version[\"serialized_model\"] = keras_model.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model.save_weights(\"model_weights.h5\")\n",
    "model_version[\"model_weights\"].upload(\"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Neptune) Wait for all operations to sync with Neptune servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version.sync()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Neptune) Promote best model to production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Neptune) Fetch current production model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with neptune.init_model(with_id=f\"{project_key}-KER\") as model:\n",
    "    model_versions_df = model.fetch_model_versions_table().to_pandas()\n",
    "model_versions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "production_models = model_versions_df[model_versions_df[\"sys/stage\"] == \"production\"][\"sys/id\"]\n",
    "assert (\n",
    "    len(production_models) == 1\n",
    "), f\"Multiple model versions found in production: {production_models.values}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prod_model_id = production_models.values[0]\n",
    "print(f\"Current model in production: {prod_model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npt_prod_model = neptune.init_model_version(with_id=prod_model_id)\n",
    "npt_prod_model_params = npt_prod_model[\"params\"].fetch()\n",
    "prod_model = tf.keras.models.model_from_json(\n",
    "    npt_prod_model[\"serialized_model\"].fetch(), custom_objects=None\n",
    ")\n",
    "\n",
    "npt_prod_model[\"model_weights\"].download()\n",
    "prod_model.load_weights(\"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Neptune) Evaluate current model on lastest test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the model's original loss and optimizer, but the current metric\n",
    "prod_model.compile(\n",
    "    loss=npt_prod_model_params[\"loss\"],\n",
    "    optimizer=npt_prod_model_params[\"optimizer\"],\n",
    "    metrics=model_params[\"metrics\"],\n",
    ")\n",
    "\n",
    "_, prod_model_acc = prod_model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Neptune) If challenger model outperforms production model, promote it to production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Production model accuracy: {prod_model_acc}\\nChallenger model accuracy: {curr_model_acc}\")\n",
    "\n",
    "if curr_model_acc > prod_model_acc:\n",
    "    print(\"Promoting challenger to production\")\n",
    "    npt_prod_model.change_stage(\"archived\")\n",
    "    model_version.change_stage(\"production\")\n",
    "else:\n",
    "    print(\"Archiving challenger model\")\n",
    "    model_version.change_stage(\"archived\")\n",
    "\n",
    "npt_prod_model.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Neptune) Stop tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_version.stop()\n",
    "run.stop()\n",
    "project.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "neptune": {
   "notebookId": "9828a187-2d06-4d81-847c-61066b3f0790",
   "projectVersion": 2
  },
  "vscode": {
   "interpreter": {
    "hash": "a9715cf0b0024f6e1c62cb31a4f1f43970eb41991212681878768b4bfe53050a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
